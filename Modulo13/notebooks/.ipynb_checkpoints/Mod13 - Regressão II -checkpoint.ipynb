{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Regressão múltipla\n",
    "\n",
    "<br>\n",
    "\n",
    "### Índice <a name=\"topo\"></a>\n",
    "\n",
    "1. [Simulando a distribuição de $\\hat{\\beta}$](#1)\n",
    "\n",
    "\n",
    "2. [Testando hipóteses sobre os parâmetros](#2)\n",
    "\n",
    "\n",
    "3. [Variáveis Qualitativas](#3)\n",
    "\n",
    "\n",
    "4. [Qualidade do modelo](#4)\n",
    "    - $R^2$\n",
    "    - AIC\n",
    "    - $R^2_{ajustado}$\n",
    "\n",
    "5. [Seleção de variáveis](#5)\n",
    "    - forward\n",
    "    - backward\n",
    "    - stepwise  \n",
    "\n",
    "6. [Regularização](#6)\n",
    "    - L1 lasso\n",
    "    - L2 ridge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from scipy.stats import ks_2samp\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simulando a distribuição de $\\hat{\\beta}$</span><a name=\"1\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Vamos 50 observações X e y, com uma associação pré-determinada, seguindo a seguinte equação:\n",
    "\n",
    "$y = 5 + 0.1 x + \\epsilon$\n",
    "\n",
    "com o parâmetro aleatório de erro sendo: $\\epsilon \\thicksim N(0,0.5)$\n",
    "\n",
    "Usando a biblioteca ```random``` do numpy é bem fácil simular estes dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(6, 4))\n",
    "\n",
    "N = 50\n",
    "\n",
    "\n",
    "x = np.linspace(0,8,N)\n",
    "y = 5 + .1*x + np.random.randn(N)*.5\n",
    "\n",
    "df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "_ = sns.regplot(x='x', y='y', data = df1, ax = ax)\n",
    "ax.set_xlim(0, 8)\n",
    "ax.set_ylim(3, 8)\n",
    "ticks = ax.set_xticks(list(range(0,9,1)))\n",
    "ticks = ax.set_yticks(list(range(3,9,1)))\n",
    "\n",
    "print(df1.corr())\n",
    "\n",
    "reg = smf.ols('y ~ x', data = df1).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variações aleatórias\n",
    "\n",
    "Nessa situação, podemos considerar que estamos extraindo 50 observações das variáveis x e y de forma aleatória de uma população com as característias especificadas.\n",
    "\n",
    "Observe que a cada vez que rodamos a célula acima, obtemos um valor distinto de $\\beta$.\n",
    "\n",
    "Vamos fazer isso algumas vezes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = .1*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(20, 10))\n",
    "g = sns.displot([betas], binwidth = .005, height = 5, aspect = 1.5)\n",
    "g.set(xlim=(0, 0.2), ylim=(0, 170))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os valores de $\\beta$ estão em torno do verdadeiro valor (0,1), embora sejam aleatórios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulando sob $H_0$\n",
    "A célula abaixo simula os dados com $\\beta = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = 0*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n",
    "    \n",
    "plt.rc('figure', figsize=(20, 10))\n",
    "g = sns.displot([betas], binwidth = .005, height = 5, aspect = 1.5)\n",
    "g.set(xlim=(-0.15, 0.15), ylim=(0, 170))\n",
    "\n",
    "# sns.displot(betas, binwidth = .005, height = 5, aspect = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que os valores obtidos dos $\\beta$s se concentram em torno do 0,1, e a frequência diminui quanto mais nos afastamos do valor verdadeiro. Essa distribuição tem a \"cara\" de uma distribuição muito conhecida e presente em diversas situações, a distribuição Normal (ou Gaussiana). E sim, sob determinadas circunstâncias, a distribuição do $\\beta$ é de fato Normal e com um desvio padrão conhecido. O desvio padrão de um parâmetro em geral é chamado por outro nome **erro padrão**, e é ele que aparece na saída do statsmodels com o nome de ``` std err ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testando hipóteses sobre os parâmetros</span><a name=\"2\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Queremos saber se uma variável é relevante. Em geral, transformamos as nossas hipóteses para uma afirmação falseável, e sob a qual conseguimos calcular probabilidades. Dessa forma, podemos formular a seguinte hipótese:\n",
    "\n",
    "$H_0: \\beta = 0$  \n",
    "$H_a: \\beta \\neq 0$\n",
    "\n",
    "Assim, sob $H_0$ temos que $\\hat{\\beta}$ dividido pela estimativa do seu erro padrão (*std err*) tem uma distribuição *t-Student* (que é bem parecida com a normal) centrada em zero. **Esse valor corresponde à coluna ```t```** na saída do statsmodels.    \n",
    "\n",
    "Em termos práticos significa que se $\\beta$ está muito longe do zero (comparado com o seu erro padrão), rejeitamos $H_0$, ou seja, se $H_0$ é falsa, significa que $H_a$ é verdadeira, ou seja, $\\beta$ é diferente de zero, o que significa que a variável é relevante no modelo de regressão. \n",
    "\n",
    "Caso contrário, não consideramos a variável relevante no modelo e ela pode ser retirada do modelo.\n",
    "\n",
    "Se você observou um valor $\\hat{\\beta}_{obs}$ como estimativa do seu beta, uma quantidade muito útil de se calcular é $p(|\\hat{\\beta}|>\\hat{\\beta}_{obs})$\n",
    "\n",
    "Um valor interessante na saída do statsmodels é o ```p>|t|```. Vamos lá: \n",
    "\n",
    "- **$\\hat{\\beta}$ é variável aleatória** ok? Primeiro entenda isso. Ele é uma função dos dados. Como os dados são variáveis aleatórias, qualquer função deles é variável aleatória também. Então ele é variável aleatória.\n",
    "- O **p-value** é a probabilidade de obtermos um $\\hat{\\beta}$ mais extremo (maior em valores absolutos) que o observado na nossa amostra, sob $H_0$.\n",
    "- **Regra prática**: então se o *p-value* é muito pequeno, digamos menor que $(1-\\gamma)$, **rejeitamos $H_0$** pois é muito pouco provável observar um beta como estes que observamos sob $H_0$ (Lembra... $H_0$ indica que $\\beta=0$ e a variável é irrelevante no modelo). Esse $\\gamma$ é o que chamamos de confiança e o $(1-\\gamma)$ de significância. Este é o famoso teste de significância aplicado à regressão.\n",
    "- **Regra de bolso**: muitas pessoas usam 5\\% como referência para o *p-value*, outras usam 1\\%. Esse assunto realmente dá pano pra manga e não vamos entrar na polêmica aqui. Mas em todo caso, lembre-se da frase do Box, de que \"todo modelo está errado\" inclusive o seu (e o meu também). A pergunta é o que torna ele útil? E o *p-value* é sem dúvida um valor muito útil.\n",
    "\n",
    "Vamos ver novamente isso no statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "\n",
    "\n",
    "x = np.linspace(0,8,N)\n",
    "y = 5 + .1*x + np.random.randn(N)*.5\n",
    "\n",
    "df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "\n",
    "_ = sns.regplot(x='x', y='y', data = df1)\n",
    "print(df1.corr())\n",
    "\n",
    "reg = smf.ols('y ~ x', data = df1).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos simular um caso em que $H_0$ é verdadeira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = []\n",
    "for i in range(2000):\n",
    "    x = np.linspace(0,8,N)\n",
    "    y = 0*x + np.random.randn(N)*.5\n",
    "    df1 = pd.DataFrame({'x':x, 'y':y})\n",
    "    reg = smf.ols('y ~ x', data = df1).fit()\n",
    "    betas.append(reg.params[1])\n",
    "    \n",
    "sns.displot(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observação sobre o teste de significância\n",
    "Repare que os valores simulados de $\\beta$ se distribuem ao longo do verdadeiro valor, que é o zero neste caso, e chegam bem próximoes de 0.1 e -0.1. Se fazemos o nosso teste com 5% de significância, quer dizer que 5% das vezes (1 em cada 20) $H_0$ vai ser verdadeira, mas vamos ter a conclusão errada. Esse é o famoso **erro tipo I** dos testes de hipóteses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variáveis qualitativas </span><a name=\"3\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Já vimos que para tratar variáveis qualitativas precisamos transformá-las em *dummies*, processo este conhecido como *\"hot encoding\"* ou simplesmente *\"encoding\"*.\n",
    "\n",
    "Antes de partir para o próximo tema, vamos mergulhar um pouco mais fundo no entendimento das variáveis *dummy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "      <th>tip_pct</th>\n",
       "      <th>net_bill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>15.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "      <td>0.191244</td>\n",
       "      <td>8.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "      <td>0.199886</td>\n",
       "      <td>17.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.162494</td>\n",
       "      <td>20.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "      <td>0.172069</td>\n",
       "      <td>20.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size   tip_pct  net_bill\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2  0.063204     15.98\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3  0.191244      8.68\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3  0.199886     17.51\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2  0.162494     20.37\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4  0.172069     20.98"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])\n",
    "tips['net_bill'] = tips['total_bill'] - tips['tip']\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='size', ylabel='tip'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pointplot(y = 'tip', x = 'size', data = tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (244, 6)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'C(size)[T.2]',\n",
       "     'C(size)[T.3]',\n",
       "     'C(size)[T.4]',\n",
       "     'C(size)[T.5]',\n",
       "     'C(size)[T.6]']\n",
       "  Terms:\n",
       "    'Intercept' (column 0), 'C(size)' (columns 1:6)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size)', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size\n",
       "0     1\n",
       "1     2\n",
       "2     3\n",
       "3     5\n",
       "4     4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'size': [1, 2, 3, 5, 4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (5, 5)\n",
       "  Intercept  C(size)[T.2]  C(size)[T.3]  C(size)[T.4]  C(size)[T.5]\n",
       "          1             0             0             0             0\n",
       "          1             1             0             0             0\n",
       "          1             0             1             0             0\n",
       "          1             0             0             0             1\n",
       "          1             0             0             1             0\n",
       "  Terms:\n",
       "    'Intercept' (column 0)\n",
       "    'C(size)' (columns 1:5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import dmatrix\n",
    "dmatrix(\"C(size)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (244, 6)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'C(size)[T.2]',\n",
       "     'C(size)[T.3]',\n",
       "     'C(size)[T.4]',\n",
       "     'C(size)[T.5]',\n",
       "     'C(size)[T.6]']\n",
       "  Terms:\n",
       "    'Intercept' (column 0), 'C(size)' (columns 1:6)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size)', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 18 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>2.17e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:23:36</td>     <th>  Log-Likelihood:    </th> <td> -390.09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   792.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   238</td>      <th>  BIC:               </th> <td>   813.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>    <td>    1.4375</td> <td>    0.606</td> <td>    2.372</td> <td> 0.018</td> <td>    0.244</td> <td>    2.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.2]</th> <td>    1.1448</td> <td>    0.614</td> <td>    1.865</td> <td> 0.063</td> <td>   -0.064</td> <td>    2.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.3]</th> <td>    1.9557</td> <td>    0.637</td> <td>    3.070</td> <td> 0.002</td> <td>    0.701</td> <td>    3.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.4]</th> <td>    2.6979</td> <td>    0.638</td> <td>    4.229</td> <td> 0.000</td> <td>    1.441</td> <td>    3.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.5]</th> <td>    2.5905</td> <td>    0.813</td> <td>    3.186</td> <td> 0.002</td> <td>    0.989</td> <td>    4.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.6]</th> <td>    3.7875</td> <td>    0.857</td> <td>    4.420</td> <td> 0.000</td> <td>    2.099</td> <td>    5.476</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>77.409</td> <th>  Durbin-Watson:     </th> <td>   1.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 244.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.344</td> <th>  Prob(JB):          </th> <td>6.87e-54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.106</td> <th>  Cond. No.          </th> <td>    24.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.249\n",
       "Model:                            OLS   Adj. R-squared:                  0.233\n",
       "Method:                 Least Squares   F-statistic:                     15.75\n",
       "Date:                Wed, 18 Aug 2021   Prob (F-statistic):           2.17e-13\n",
       "Time:                        11:23:36   Log-Likelihood:                -390.09\n",
       "No. Observations:                 244   AIC:                             792.2\n",
       "Df Residuals:                     238   BIC:                             813.2\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "Intercept        1.4375      0.606      2.372      0.018       0.244       2.631\n",
       "C(size)[T.2]     1.1448      0.614      1.865      0.063      -0.064       2.354\n",
       "C(size)[T.3]     1.9557      0.637      3.070      0.002       0.701       3.211\n",
       "C(size)[T.4]     2.6979      0.638      4.229      0.000       1.441       3.955\n",
       "C(size)[T.5]     2.5905      0.813      3.186      0.002       0.989       4.192\n",
       "C(size)[T.6]     3.7875      0.857      4.420      0.000       2.099       5.476\n",
       "==============================================================================\n",
       "Omnibus:                       77.409   Durbin-Watson:                   1.807\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              244.825\n",
       "Skew:                           1.344   Prob(JB):                     6.87e-54\n",
       "Kurtosis:                       7.106   Cond. No.                         24.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(y, x).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5823"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.4375 + 1.1448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (244, 6)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'C(size, Treatment(2))[T.1]',\n",
       "     'C(size, Treatment(2))[T.3]',\n",
       "     'C(size, Treatment(2))[T.4]',\n",
       "     'C(size, Treatment(2))[T.5]',\n",
       "     'C(size, Treatment(2))[T.6]']\n",
       "  Terms:\n",
       "    'Intercept' (column 0), 'C(size, Treatment(2))' (columns 1:6)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, x = patsy.dmatrices('tip ~ C(size, Treatment(2))', data = tips)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.75</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 18 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>2.17e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:27:59</td>     <th>  Log-Likelihood:    </th> <td> -390.09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   792.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   238</td>      <th>  BIC:               </th> <td>   813.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>    2.5823</td> <td>    0.097</td> <td>   26.613</td> <td> 0.000</td> <td>    2.391</td> <td>    2.773</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.1]</th> <td>   -1.1448</td> <td>    0.614</td> <td>   -1.865</td> <td> 0.063</td> <td>   -2.354</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.3]</th> <td>    0.8109</td> <td>    0.219</td> <td>    3.698</td> <td> 0.000</td> <td>    0.379</td> <td>    1.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.4]</th> <td>    1.5531</td> <td>    0.222</td> <td>    7.008</td> <td> 0.000</td> <td>    1.117</td> <td>    1.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.5]</th> <td>    1.4457</td> <td>    0.551</td> <td>    2.626</td> <td> 0.009</td> <td>    0.361</td> <td>    2.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.6]</th> <td>    2.6427</td> <td>    0.614</td> <td>    4.306</td> <td> 0.000</td> <td>    1.434</td> <td>    3.852</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>77.409</td> <th>  Durbin-Watson:     </th> <td>   1.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 244.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.344</td> <th>  Prob(JB):          </th> <td>6.87e-54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.106</td> <th>  Cond. No.          </th> <td>    8.26</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.249\n",
       "Model:                            OLS   Adj. R-squared:                  0.233\n",
       "Method:                 Least Squares   F-statistic:                     15.75\n",
       "Date:                Wed, 18 Aug 2021   Prob (F-statistic):           2.17e-13\n",
       "Time:                        11:27:59   Log-Likelihood:                -390.09\n",
       "No. Observations:                 244   AIC:                             792.2\n",
       "Df Residuals:                     238   BIC:                             813.2\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                      2.5823      0.097     26.613      0.000       2.391       2.773\n",
       "C(size, Treatment(2))[T.1]    -1.1448      0.614     -1.865      0.063      -2.354       0.064\n",
       "C(size, Treatment(2))[T.3]     0.8109      0.219      3.698      0.000       0.379       1.243\n",
       "C(size, Treatment(2))[T.4]     1.5531      0.222      7.008      0.000       1.117       1.990\n",
       "C(size, Treatment(2))[T.5]     1.4457      0.551      2.626      0.009       0.361       2.530\n",
       "C(size, Treatment(2))[T.6]     2.6427      0.614      4.306      0.000       1.434       3.852\n",
       "==============================================================================\n",
       "Omnibus:                       77.409   Durbin-Watson:                   1.807\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              244.825\n",
       "Skew:                           1.344   Prob(JB):                     6.87e-54\n",
       "Kurtosis:                       7.106   Cond. No.                         8.26\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.OLS(y, x).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Qualidade do modelo e complexidade</span><a name=\"4\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Quando fazemos uma regressão múltipla, pelo próprio método de mínimos quadrados ordinários, a métrica $R^2$ vai ser necessariamente melhor sempre que adicionarmos uma variável a mais. Sempre. Por menos sentido que a variável faça, por menos informação que ela agregue, o $R^2$ vai ser maior (ou no pior extremo caso, igual) ao que tínhamos antes.\n",
    "\n",
    "Vamos ver isso na prática na base de gorjetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   21.03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 18 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>9.61e-20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:31:49</td>     <th>  Log-Likelihood:    </th> <td> -372.87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   759.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   237</td>      <th>  BIC:               </th> <td>   784.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   -0.3279</td> <td>    0.494</td> <td>   -0.664</td> <td> 0.508</td> <td>   -1.301</td> <td>    0.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.1]</th> <td>   -0.1059</td> <td>    0.599</td> <td>   -0.177</td> <td> 0.860</td> <td>   -1.285</td> <td>    1.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.3]</th> <td>    0.4012</td> <td>    0.216</td> <td>    1.859</td> <td> 0.064</td> <td>   -0.024</td> <td>    0.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.4]</th> <td>    0.8693</td> <td>    0.236</td> <td>    3.679</td> <td> 0.000</td> <td>    0.404</td> <td>    1.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.5]</th> <td>    0.6797</td> <td>    0.530</td> <td>    1.283</td> <td> 0.201</td> <td>   -0.364</td> <td>    1.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.6]</th> <td>    1.7283</td> <td>    0.593</td> <td>    2.914</td> <td> 0.004</td> <td>    0.560</td> <td>    2.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(net_bill)</th>           <td>    1.1401</td> <td>    0.190</td> <td>    5.993</td> <td> 0.000</td> <td>    0.765</td> <td>    1.515</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>74.766</td> <th>  Durbin-Watson:     </th> <td>   1.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 231.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.303</td> <th>  Prob(JB):          </th> <td>5.05e-51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.999</td> <th>  Cond. No.          </th> <td>    28.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.347\n",
       "Model:                            OLS   Adj. R-squared:                  0.331\n",
       "Method:                 Least Squares   F-statistic:                     21.03\n",
       "Date:                Wed, 18 Aug 2021   Prob (F-statistic):           9.61e-20\n",
       "Time:                        12:31:49   Log-Likelihood:                -372.87\n",
       "No. Observations:                 244   AIC:                             759.7\n",
       "Df Residuals:                     237   BIC:                             784.2\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     -0.3279      0.494     -0.664      0.508      -1.301       0.645\n",
       "C(size, Treatment(2))[T.1]    -0.1059      0.599     -0.177      0.860      -1.285       1.074\n",
       "C(size, Treatment(2))[T.3]     0.4012      0.216      1.859      0.064      -0.024       0.826\n",
       "C(size, Treatment(2))[T.4]     0.8693      0.236      3.679      0.000       0.404       1.335\n",
       "C(size, Treatment(2))[T.5]     0.6797      0.530      1.283      0.201      -0.364       1.724\n",
       "C(size, Treatment(2))[T.6]     1.7283      0.593      2.914      0.004       0.560       2.897\n",
       "np.log(net_bill)               1.1401      0.190      5.993      0.000       0.765       1.515\n",
       "==============================================================================\n",
       "Omnibus:                       74.766   Durbin-Watson:                   1.966\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              231.626\n",
       "Skew:                           1.303   Prob(JB):                     5.05e-51\n",
       "Kurtosis:                       6.999   Cond. No.                         28.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = smf.ols('tip ~ C(size, Treatment(2)) + np.log(net_bill)', data = tips).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos inserir a variavel *day* e checar os resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 18 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>6.16e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:33:41</td>     <th>  Log-Likelihood:    </th> <td> -372.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   765.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   234</td>      <th>  BIC:               </th> <td>   800.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   -0.3904</td> <td>    0.504</td> <td>   -0.775</td> <td> 0.439</td> <td>   -1.383</td> <td>    0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.1]</th> <td>   -0.1007</td> <td>    0.605</td> <td>   -0.166</td> <td> 0.868</td> <td>   -1.293</td> <td>    1.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.3]</th> <td>    0.3880</td> <td>    0.221</td> <td>    1.757</td> <td> 0.080</td> <td>   -0.047</td> <td>    0.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.4]</th> <td>    0.8480</td> <td>    0.242</td> <td>    3.510</td> <td> 0.001</td> <td>    0.372</td> <td>    1.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.5]</th> <td>    0.6527</td> <td>    0.536</td> <td>    1.217</td> <td> 0.225</td> <td>   -0.404</td> <td>    1.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size, Treatment(2))[T.6]</th> <td>    1.7578</td> <td>    0.604</td> <td>    2.911</td> <td> 0.004</td> <td>    0.568</td> <td>    2.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Fri]</th>                 <td>    0.1562</td> <td>    0.301</td> <td>    0.520</td> <td> 0.604</td> <td>   -0.436</td> <td>    0.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Sat]</th>                 <td>    0.0385</td> <td>    0.195</td> <td>    0.197</td> <td> 0.844</td> <td>   -0.346</td> <td>    0.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Sun]</th>                 <td>    0.1393</td> <td>    0.202</td> <td>    0.689</td> <td> 0.491</td> <td>   -0.259</td> <td>    0.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(net_bill)</th>           <td>    1.1395</td> <td>    0.192</td> <td>    5.931</td> <td> 0.000</td> <td>    0.761</td> <td>    1.518</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>75.986</td> <th>  Durbin-Watson:     </th> <td>   1.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 240.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.318</td> <th>  Prob(JB):          </th> <td>7.65e-53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.082</td> <th>  Cond. No.          </th> <td>    28.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.349\n",
       "Model:                            OLS   Adj. R-squared:                  0.324\n",
       "Method:                 Least Squares   F-statistic:                     13.96\n",
       "Date:                Wed, 18 Aug 2021   Prob (F-statistic):           6.16e-18\n",
       "Time:                        12:33:41   Log-Likelihood:                -372.52\n",
       "No. Observations:                 244   AIC:                             765.0\n",
       "Df Residuals:                     234   BIC:                             800.0\n",
       "Df Model:                           9                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     -0.3904      0.504     -0.775      0.439      -1.383       0.602\n",
       "C(size, Treatment(2))[T.1]    -0.1007      0.605     -0.166      0.868      -1.293       1.092\n",
       "C(size, Treatment(2))[T.3]     0.3880      0.221      1.757      0.080      -0.047       0.823\n",
       "C(size, Treatment(2))[T.4]     0.8480      0.242      3.510      0.001       0.372       1.324\n",
       "C(size, Treatment(2))[T.5]     0.6527      0.536      1.217      0.225      -0.404       1.709\n",
       "C(size, Treatment(2))[T.6]     1.7578      0.604      2.911      0.004       0.568       2.947\n",
       "day[T.Fri]                     0.1562      0.301      0.520      0.604      -0.436       0.749\n",
       "day[T.Sat]                     0.0385      0.195      0.197      0.844      -0.346       0.423\n",
       "day[T.Sun]                     0.1393      0.202      0.689      0.491      -0.259       0.537\n",
       "np.log(net_bill)               1.1395      0.192      5.931      0.000       0.761       1.518\n",
       "==============================================================================\n",
       "Omnibus:                       75.986   Durbin-Watson:                   1.968\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              240.004\n",
       "Skew:                           1.318   Prob(JB):                     7.65e-53\n",
       "Kurtosis:                       7.082   Cond. No.                         28.8\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = smf.ols('tip ~ C(size, Treatment(2)) + np.log(net_bill) + day', data = tips).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observações\n",
    "\n",
    "- O $R^2$ aumentou, embora a variável adicionada não pareça ser significante.\n",
    "- O $R^2$ sempre vai aumentar. Na pior das hipóteses ele fica igual.\n",
    "- O modelo ficou mais \"complicado\".\n",
    "- Estamos aumentando o risco de \"overfitting\".\n",
    "- Esta variável adicional interfere nas estimativas dos demais parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navalha de Occam\n",
    "\n",
    "Um princípio conhecido como *[Navalha de Occam](https://en.wikipedia.org/wiki/Occam%27s_razor)* indica que se temos dois modelos com indicadores iguais de qualidade, e um é mais simples, o mais simples é desejável. Dessa forma, diversas propostas surgem na tentativa de \"balisar\" a quantidade de parâmetros no modelo, como o $R^2$-*ajustado* - que sofre uma penalização por cada parâmetro no modelo e o AIC que vamos discutir adiante.\n",
    "\n",
    "Com isso em mente, há na literatura diversas alternativas para se considerar a complexidade do modelo na medida de qualidade, como o critério de Akaike (AIC) e o $R^2-ajustado$.\n",
    "\n",
    "#### AIC\n",
    "\n",
    "*Akaike´s Information Criterion* (ou critério da informação de Akaike). É uma métrica mais \"estatística\" de qualidade de ajuste do modelo, desenhada para comparar modelos com diferentes combinações de variáveis. Quanto menor o AIC, melhor o modelo - ou seja, se colocamos uma nova variável no modelo, por esse critério ela é relevante se o AIC diminuir, e não é relevante caso contrário. \n",
    "\n",
    "Diferente do $R^2$, o AIC depende do tamanho da amostra, de modo que não tem uma 'regra de bolso' do tipo \"perto de 1 é bom\", mas é adequado para comparar modelos na mesma amostra.\n",
    "\n",
    "A Wikipedia tem um artigo interessante sobre o [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion).\n",
    "\n",
    "#### $R^2-ajustado$\n",
    "\n",
    "O $R^2$-ajustado procura ponderar o incremento em explicação da variabilidade com o incremento em complexidade do modelo em termos de número de parâmetros. Ele aumenta se o $R^2$ aumentar mais do que o esperado \"por acaso\", e diminui caso contrário. Sua fórmula é a seguinte:\n",
    "\n",
    "$$R^2_{aj} = 1- \\left[ \\frac{(1-R^2)(n-1)}{(n-k-1)} \\right]$$\n",
    "\n",
    "#### Observações do exemplo anterior\n",
    "Repare que, no exercício anterior, quando inserimos uma variável irrelevante no modelo, o $R^2$ aumentou, mas o $R^2-ajustado$ diminuiu e o AIC aumentou, sugerindo que esta variável não deve entrar no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Seleção de modelos </span><a name=\"5\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Três algoritmos clássicos na literatura estatística para seleção de variáveis:\n",
    "\n",
    "- *Forward*:  \n",
    "    Parte de um modelo vazio e vai incluindo variáveis estatisticamente relevantes uma a uma, priorizando a mais relevante, até que nenhuma seja incluída. Pode haver alguma variável que deixou de ser relevante na presença daquelas que foram incluídas depois.\n",
    "    1. Definir um limite *LI* de *p-value* para uma variável ser **incluída** no modelo\n",
    "    2. Iniciar com um modelo sem variáveis\n",
    "    3. Para cada variável fora do modelo, testar $\\beta = 0$ na presença das demais - armazenar o *p-value*\n",
    "    4. Se o menor *p-value* for menor que *L*, a variável correspondente é incluída no modelo\n",
    "    5. Repetir 3 e 4 até que não sejam adicionadas variáveis ao modelo\n",
    "    <br><br>\n",
    "- *Backward*:  \n",
    "    Parte de um modelo com todas as variáveis possíveis consideradas e vai removendo-as uma a uma, até que nenhuma seja removida. Pode haver variáveis relevantes ainda após o término.\n",
    "    1. Definir um limite *LE* de *p-value* para uma variável ser **excluída** do modelo\n",
    "    2. Para cada variável incluída no modelo, testar $\\beta = 0$ na presença das demais - armazenar o *p-value*\n",
    "    3. Se o menor *p-value* for maior que *LE*, a variável é excluída do modelo\n",
    "    5. Repetir 3 e 4 até que não sejam excluídas mais variáveis do modelo\n",
    "- *Stepwise*:\n",
    "    É básicamente uma mistura dos dois. Vai incluindo variáveis, eventualmente removendo alguma variável caso seja irrelevante na presença das demais.\n",
    "    \n",
    "**Crítica**: Essa abordagem é criticada na comunidade porque esse *p-value* é tido mais como uma referência. Muitos usam esse algoritmo com o critério de Akaike ao invés do *p-value*, ou mesmo as regularizações L1 e L2, com a qual é possível fazer um *grid search* para buscar melhores resultados em previsão.\n",
    "\n",
    "De qualquer forma, a seleção de um modelo por um desses algoritmos **muito raramente** (pra não dizer nunca) é a escolha final. Sempre há insights e ajustes a serem feitos, categorias a agrupar, variáveis conceitualmente importantes que podem ser priorizadas, multicolinearidade a ser tratada (mais sobre isso adiante), enfim, é um processo meio arte meio ciência suportado por algoritmos menos que executado por algoritmos.\n",
    "\n",
    "O código abaixo foi extraído e adaptado do fórum [*stackovervlow*](https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm), da resposta do David Dale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  LSTAT                          with p-value 5.0811e-88\n",
      "#############\n",
      "['LSTAT']\n",
      "Add  RM                             with p-value 3.47226e-27\n",
      "#############\n",
      "['LSTAT', 'RM']\n",
      "Add  PTRATIO                        with p-value 1.64466e-14\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO']\n",
      "Add  DIS                            with p-value 1.66847e-05\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS']\n",
      "Add  NOX                            with p-value 5.48815e-08\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX']\n",
      "Add  CHAS                           with p-value 0.000265473\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS']\n",
      "Add  B                              with p-value 0.000771946\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B']\n",
      "Add  ZN                             with p-value 0.00465162\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN']\n",
      "Add  CRIM                           with p-value 0.0445675\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM']\n",
      "Add  RAD                            with p-value 0.00169218\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD']\n",
      "Add  TAX                            with p-value 0.000521424\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n",
      "#############\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n",
      "resulting features:\n",
      "['LSTAT', 'RM', 'PTRATIO', 'DIS', 'NOX', 'CHAS', 'B', 'ZN', 'CRIM', 'RAD', 'TAX']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype=np.dtype('float64'))\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.index[new_pval.argmin()]\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        print(\"#############\")\n",
    "        print(included)\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "variaveis = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(variaveis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Regularização </span><a name=\"6\"></a>\n",
    "[Voltar ao índice](#topo)\n",
    "\n",
    "Regularização (ou *model regularization*) é uma forma de considerar a complexidade adicionada ao modelo e simplificar o modelo, quer seja por deixar os parâmetros menos relevantes, quer seja por retirá-los intrgralmente do modelo.\n",
    "\n",
    "As duas formas mais populares na literatura do aprendizado de máquina são a regularização L1 e a regularização L2:\n",
    "\n",
    "#### Função de perda\n",
    "Vamos relembrar que a nossa regressão é uma regressãod e mínimos quadrados, ou seja, estamos minimizando a função do erro quadrático médio (EQM) em função dos parâmetros do modelo ($\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$). Nossa função de erro, podemos chamá-la de um nome mais geral: *função de perda* L:\n",
    "\n",
    "$$L = \\sum_{n=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2$$\n",
    "\n",
    "As formas de regularização mais populares introduzem uma \"penalização\" na função de perda devido ao aumento na complexidade do modelo - isto é, devido ao aumento do número de parâmetros (ou variáveis) no modelo.\n",
    "\n",
    "#### Regularização L1 (lasso)\n",
    "A regressão lasso introduz uma penalidade igual ao quadrado da soma dos coeficientes na função de perda:\n",
    "\n",
    "$$L_1 = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\sum_{k=0}^{M} \\left| \\beta_k \\right|$$\n",
    "\n",
    "Em que:  \n",
    "- $\\beta_k$ são os parâmetros do modelo (atenção que $\\beta_0$ é o intercepto).\n",
    "- N é o número de observações\n",
    "- M é o número de parâmetros\n",
    "- $\\alpha$ no statsmodels é um *hiperparâmetro* do modelo, que regula a penalidade por complexidade.\n",
    "\n",
    "Dessa forma, minimizando essa função de perda, os parâmetros do modelo tendem a ter valor absoluto menor, e caso tragam mais complexidade que explicação da variância, são \"zerados\", o que significa que a variável correspondente fica é eliminada do modelo.\n",
    "\n",
    "#### Regularização L2 (ridge)\n",
    "Outra forma de regularização é a chamada regularização *ridge*, que minimiza a seguinte perda:\n",
    "\n",
    "$$L_2 = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 + \\alpha \\sum_{k=0}^{M} \\left| \\beta_k \\right|^2$$\n",
    "\n",
    "Em que:  \n",
    "- $\\beta_k$ são os parâmetros do modelo (atenção que $\\beta_0$ é o intercepto).\n",
    "- N é o número de observações\n",
    "- M é o número de parâmetros\n",
    "- $\\alpha$ no statsmodels é um *hiperparâmetro* do modelo, que regula a penalidade por complexidade.\n",
    "\n",
    "Essa regularização é semelhante ao *lasso*, porém a penalização é no valor absoluto dos parâmetros. Diferente do lasso, não costuma \"zerar\" os parâmetros das variáveis menos relevantes, somente reduzir os coeficientes.\n",
    "\n",
    "#### *Elastic net*\n",
    "Uma forma bem popular de regularização de regressão é o *elastic net*, que consiste na mistura dos dois otimizando a seguinte função de perda:\n",
    "\n",
    "$$L_E = \\sum_{i=1}^{N} \\left( y_i - \\hat{y_i} \\right)^2 \n",
    "    + \\alpha \\left( L1_{wt} \\sum_{k=0}^{M} \\left| \\beta_k \\right|\n",
    "                    + (1-L1_{wt}) \\sum_{k=0}^{M} \\left( \\beta_k \\right)^2\n",
    "             \\right)$$\n",
    "\n",
    "com:  \n",
    "- N é o número de observações e M o número de parâmetros\n",
    "- $\\alpha$ sendo o hiperparâmetro que dá importância à penalização  \n",
    "- $L1_{wt}$ sendo um número entre 0 e 1 \n",
    "    - quando vale 1, equivale regulaziração L1 - lasso\n",
    "    - quando 0 equivale a L2 - ridge\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos testar\n",
    "\n",
    "Vamos usar o Lasso, pois é uma forma interessante de fazer seleção de variáveis no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>tip</td>       <th>  R-squared:         </th> <td>   0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.72</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 18 Aug 2021</td> <th>  Prob (F-statistic):</th> <td>1.62e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:56:56</td>     <th>  Log-Likelihood:    </th> <td> -372.83</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   244</td>      <th>  AIC:               </th> <td>   763.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   236</td>      <th>  BIC:               </th> <td>   795.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>   -0.0943</td> <td>    0.553</td> <td>   -0.171</td> <td> 0.865</td> <td>   -1.183</td> <td>    0.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.2]</th>     <td>   -0.3591</td> <td>    0.197</td> <td>   -1.819</td> <td> 0.070</td> <td>   -0.748</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.3]</th>     <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.4]</th>     <td>    0.4563</td> <td>    0.256</td> <td>    1.782</td> <td> 0.076</td> <td>   -0.048</td> <td>    0.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.5]</th>     <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(size)[T.6]</th>     <td>    1.3854</td> <td>    0.610</td> <td>    2.270</td> <td> 0.024</td> <td>    0.183</td> <td>    2.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoker[T.No]</th>     <td>   -0.0929</td> <td>    0.156</td> <td>   -0.596</td> <td> 0.552</td> <td>   -0.400</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time[T.Dinner]</th>   <td>    0.0376</td> <td>    0.185</td> <td>    0.203</td> <td> 0.839</td> <td>   -0.326</td> <td>    0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Fri]</th>       <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Sat]</th>       <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day[T.Sun]</th>       <td>    0.1234</td> <td>    0.178</td> <td>    0.692</td> <td> 0.490</td> <td>   -0.228</td> <td>    0.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>np.log(net_bill)</th> <td>    1.1883</td> <td>    0.178</td> <td>    6.693</td> <td> 0.000</td> <td>    0.839</td> <td>    1.538</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>72.687</td> <th>  Durbin-Watson:     </th> <td>   2.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 219.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.276</td> <th>  Prob(JB):          </th> <td>2.58e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.879</td> <th>  Cond. No.          </th> <td>    67.4</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    tip   R-squared:                       0.348\n",
       "Model:                            OLS   Adj. R-squared:                  0.328\n",
       "Method:                 Least Squares   F-statistic:                     15.72\n",
       "Date:                Wed, 18 Aug 2021   Prob (F-statistic):           1.62e-18\n",
       "Time:                        16:56:56   Log-Likelihood:                -372.83\n",
       "No. Observations:                 244   AIC:                             763.7\n",
       "Df Residuals:                     236   BIC:                             795.1\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept           -0.0943      0.553     -0.171      0.865      -1.183       0.995\n",
       "C(size)[T.2]        -0.3591      0.197     -1.819      0.070      -0.748       0.030\n",
       "C(size)[T.3]              0          0        nan        nan           0           0\n",
       "C(size)[T.4]         0.4563      0.256      1.782      0.076      -0.048       0.961\n",
       "C(size)[T.5]              0          0        nan        nan           0           0\n",
       "C(size)[T.6]         1.3854      0.610      2.270      0.024       0.183       2.588\n",
       "smoker[T.No]        -0.0929      0.156     -0.596      0.552      -0.400       0.214\n",
       "time[T.Dinner]       0.0376      0.185      0.203      0.839      -0.326       0.401\n",
       "day[T.Fri]                0          0        nan        nan           0           0\n",
       "day[T.Sat]                0          0        nan        nan           0           0\n",
       "day[T.Sun]           0.1234      0.178      0.692      0.490      -0.228       0.475\n",
       "np.log(net_bill)     1.1883      0.178      6.693      0.000       0.839       1.538\n",
       "==============================================================================\n",
       "Omnibus:                       72.687   Durbin-Watson:                   2.002\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              219.153\n",
       "Skew:                           1.276   Prob(JB):                     2.58e-48\n",
       "Kurtosis:                       6.879   Cond. No.                         67.4\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regularization\n",
    "modelo = 'tip ~ C(size) + np.log(net_bill) + smoker + time + day'\n",
    "md = smf.ols(modelo, data = tips)\n",
    "reg = md.fit_regularized(method = 'elastic_net' \n",
    "                         , refit = True\n",
    "                         , L1_wt = 1\n",
    "                         , alpha = 0.01)\n",
    "\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            print(included+[new_column])\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        print(included)\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stepwise = sm.OLS(y, sm.add_constant(pd.DataFrame(X[variaveis]))).fit()\n",
    "reg_stepwise.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Best subsets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}\n",
    "\n",
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = getBest(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models.sort_values('RSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "mu = 0\n",
    "\n",
    "sigma = 1\n",
    "x = np.linspace(-5, 5, 500)\n",
    "plt.plot(x, stats.norm.pdf(x, 0, 1))\n",
    "\n",
    "xinf = np.linspace(-5, stats.norm.ppf(.025, 0, 1), 500)\n",
    "plt.fill_between(xinf, stats.norm.pdf(xinf, 0, 1), color = 'orange')\n",
    "\n",
    "xsup = np.linspace(stats.norm.ppf(.975, 0, 1), 5 , 500)\n",
    "plt.fill_between(xsup, stats.norm.pdf(xsup, 0, 1), color = 'orange')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinf = np.linspace(-5, stats.norm.ppf(.025, 0, 1), 500)\n",
    "plt.fill_between(xinf, stats.norm.pdf(xinf, 0, 1))\n",
    "\n",
    "xsup = np.linspace(stats.norm.ppf(.975, 0, 1), 5 , 500)\n",
    "plt.fill_between(xsup, stats.norm.pdf(xsup, 0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "from patsy import balanced\n",
    "from patsy import dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patsy.dmatrix(\"C(a, Diff)\", balanced(a=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced(a=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from patsy import demo_data\n",
    "\n",
    "data = demo_data(\"a\", nlevels=3)\n",
    "l = [\"a3\", \"a2\", \"a1\"]\n",
    "\n",
    "dmatrix(\"C(a, levels=l)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = [[0, 0], [0, 1], [1, 1]]\n",
    "\n",
    "dmatrix(\"C(a, ordinal)\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
